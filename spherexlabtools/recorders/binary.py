""" This module implements Recorder sub-classes that write to binary output files.
"""

import os

import numpy as np
import pandas as pd
# from astropy.io import fits

import spherexlabtools.log as slt_log
from spherexlabtools.recorders import Recorder


class HDFRecorder(Recorder):
    """ A non-merging recorder that writes the data, procedure parameters, and metadata tables into three separate groups
    of an HDF5 file:

        1. 'data'
        2. 'proc_params'
        3. 'meta'
    """

    _data_group_str = "data"
    _pp_group_str = "proc_params"
    _meta_group_str = "meta"

    def __init__(self, cfg, exp, **kwargs):
        super().__init__(cfg, exp, extension=".h5", merge=False, **kwargs)

    def open_results(self, exists):
        """ Open existing or create a new .h5 file. If the file exists then read the latest RecordGroup and RecordGroupInd
        values from the 'proc_params' group.

        :return: The appropriate RecordGroup and RecordGroupInd values.
        """
        fp = self.results_path.value()
        self.opened_results = pd.HDFStore(fp)
        if exists:
            rec_group, rec_group_ind = self.opened_results[self._pp_group_str].index[-1]
        else:
            rec_group = -1
            rec_group_ind = -1

        return rec_group, rec_group_ind

    def close_results(self):
        """ Close the .h5 results file.
        """
        if self.opened_results is not None:
            self.opened_results.close()
            self.opened_results = None

    def update_results(self):
        """ Append to the HDF groups with the information from the new dataframes.
        """
        self.opened_results.append(self._data_group_str, self.data_df)
        self.opened_results.append(self._pp_group_str, self.pp_df)
        self.opened_results.append(self._meta_group_str, self.meta_df)


# class FITSRecorder(Recorder):
#     """ This class implements writing to .fits output, generating a new fits file for each individual
#     RecordGroup in a measurement. Files always have the record group number appended to the file-name. Thus, in the
#     case of the :ref:`SLT Data Output Example <user_guide/data_output/standard:Example>`, there would be two files
#     generated for each measurement performed. The first would have '_0' appended, while the second file would have
#     '_1' appended. Every fits file generated by this recorder contains two header-data-units (HDUs). The first HDU is
#     the *primary* HDU where the data table is written, while the second is a binary table HDU that contains all
#     information from the metadata and procedure parameters tables. In more detail, the HDUs are structured as follows:
#
#     **Primary HDU (Data table)**
#     The primary HDU data attribute is a cube array of all of the records generated in a single *RecordGroup*.
#     The cube is structured with the first dimension being the *RecordGroupInd*, the second dimension being the *RecordRow*,
#     and the final dimension being the *column* within the record. Note that for any given fits file (RecordGroup), the
#     number of *RecordRows* must remain fixed.
#
#     The header attribute of the primary HDU contains the following entries, some of which are created by default through
#     the astropy fits writer:
#
#         - SIMPLE: boolean indicating if the file conforms to the FITS standard
#         - BITPIX: data type of the array
#         - NAXIS: number of array dimensions (will always be 3)
#         - NAXIS1: number of columns in each record.
#         - NAXIS2: number of rows in each record (RecordRow).
#         - NAXIS3: number of records stored (RecordGroupInd).
#         - COL\_#: Only present if the initialization kwarg *label_datacols* is set to True (default False). Here, there
#                  will be an entry for every column present in a record where # is replaced with the column number. The
#                  contents of each entry is then the string name identifying the column.
#
#     **Extension HDU (Metadata / Procedure Parameters table)**
#     The data attribute is a table with the *procedure parameters* and *metadata* tables joined along with the *RecordGroupInd*
#     and *RecordRow* information.
#
#     The header is simply the the default header for the BinTableHDU type in astropy. The entries that are of interest
#     are the following:
#
#         - NAXIS2: number of records in the table.
#         - TFIELDS: number of table columns
#         - TTYPE#: string label of column number #.
#         - TFORM#: data type of column number #.
#
#     *note: the opened_results attribute is used to hold the primary header data unit.*
#     """
#
#     _rg_append_char = '_'
#
#     def __init__(self, cfg, exp, label_datacols=False, **kwargs):
#         """ Initialize a FITSRecorder
#
#         :param cfg: Configuration dictionary.
#         :param exp: Experiment control package.
#         :param label_datacols: Boolean indicating if the primary HDU should contain column labels in the header.
#         :param kwargs: Base recorder kwargs.
#         """
#         super().__init__(cfg, exp, extension=".fits", merge=True, **kwargs)
#         self._prev_rec_group = None
#         self.label_datacols = label_datacols
#         self.meta_pp_cols = []
#
#     def should_open(self):
#         """ Override the base should_open method to check if a directory for fits files exists, rather than a single
#         file.
#
#         :param record:
#         :return:
#         """
#         fp = self.results_path.value()
#         fp_exists = os.path.exists(fp)
#         if (not fp_exists) or self.results_path_changed:
#             ret = True
#             self.results_path_changed = False
#         else:
#             ret = False
#
#         return ret, fp_exists
#
#     def open_results(self, exists):
#         """ Open existing or create a new .fits output directory. If the directory exists, then get the latest record
#         group.
#
#         :param exists:
#         :return:
#         """
#         if not exists:
#             os.makedirs(self.results_path.value(), exist_ok=True)
#
#         self.opened_results = None
#         fits_files = [f.replace(self.extension, '') for f in os.listdir(self.results_path.value()) if self.extension in f]
#         if not exists or len(fits_files) == 0:
#             rec_group = -1
#             rec_group_ind = -1
#         else:
#             fits_files.sort()
#             rec_group = int(fits_files[-1].split(self._rg_append_char)[-1])
#             rec_group_ind = -1
#
#         self._prev_rec_group = rec_group
#         return rec_group, rec_group_ind
#
#     def close_results(self):
#         self.opened_results.close()
#
#     def update_results(self):
#
#         data_values = self.data_df.values.astype(np.float64)
#         data_values = np.array([data_values])
#
#         # - if a new record group, then open a new HDUL and write the data header - #
#         if self._prev_rec_group != self.record_group:
#             # - update the meta_pp_columns - #
#             self.meta_pp_columns = [col for col in self.merged_df.columns if 'proc' in str(col) or 'meta' in str(col)]
#
#             # - create the primary hdu - #
#             phdu = fits.PrimaryHDU(data_values)
#             if self.label_datacols:
#                 icol = 0
#                 for col in self.data_df.columns:
#                     col_str = 'COL_%i' % icol
#                     phdu.header[col_str] = col
#                     icol += 1
#
#             # - create the extension hdu - #
#             rgi_col = fits.Column(name='RecordGroupInd', format='K', array=[self.record_group_ind])
#             cols = [rgi_col]
#             for col in self.meta_pp_columns:
#                 if 'proc' in col:
#                     array = self.pp_df[col]
#                 elif 'meta' in col:
#                     array = self.meta_df[col]
#                 column = fits.Column(name=col, format='K', array=array)
#                 cols.append(column)
#             ext_hdu = fits.BinTableHDU.from_columns(cols)
#
#             # - create the HDUL - #
#             self.opened_results = fits.HDUList([phdu, ext_hdu])
#
#         # - otherwise just append to the existing HDUL - #
#         else:
#             # - append to the primary HDU - #
#             phdu_data = self.opened_results[0].data
#             phdu_data = np.concatenate([phdu_data, data_values])
#             self.opened_results[0].data = phdu_data
#
#             # - append to the extension HDU - #
#             ext_dtype = self.opened_results[1].data.dtype
#             ext_list = [self.record_group_ind]
#             for col in self.meta_pp_columns:
#                 if 'proc' in col:
#                     array = self.pp_df[col]
#                 elif 'meta' in col:
#                     array = self.meta_df[col]
#                 ext_list.extend(array)
#             ext_append = np.array([tuple(ext_list)], dtype=ext_dtype)
#             ext_data = np.concatenate([self.opened_results[1].data, ext_append])
#             self.opened_results[1].data = ext_data
#
#         # - write the fits file - #
#         path = os.path.join(self.results_path.value(), 'rg%s%04i.fits' % (self._rg_append_char, self.record_group))
#         self.opened_results.writeto(path, overwrite=True)
#
#         # - update the previous record group attribute - #
#         self._prev_rec_group = self.record_group

